SESSION CURICULUM

——>DEEPLEARNING
——————>ANN
————————>Act Func,Loss Func,Optimization Algos
——————>CNN
——————>GAN
—————————>Descriminative models and Generative models and 1 usecase.
——————>NLP(BASICS)
——————>RNN
——————>PROJECTS END TO END
—————————>1.REVERSE IMAGE SEARCH ENGINE—>(CNN)
—————————>2.CONSUMER COMPLAINT ANALYSIS—>(NLP)

--->Artificial Neural Networks (ANN):
->Introduction to Neural Networks: Understanding the basics of neural networks, neuron structure, and activation functions.
->Feedforward Neural Networks: Exploring the structure and working of feedforward neural networks, including input, hidden, and output layers.
->Activation Functions: Studying different activation functions like sigmoid, tanh, ReLU, and their applications.
->Loss Functions: Understanding various loss functions such as mean squared error, cross-entropy, and their role in training neural networks.
->Optimization Algorithms: Exploring optimization algorithms like gradient descent, stochastic gradient descent, Adam, and their impact on model training.
->Backpropagation: Understanding the backpropagation algorithm and its role in updating the weights of a neural network.
->Regularization Techniques: Exploring regularization methods like L1 and L2 regularization, dropout, and their importance in preventing overfitting.
->Hyperparameter Tuning: Techniques for tuning hyperparameters of a neural network to optimize model performance.
Transfer Learning: Leveraging pre-trained models and fine-tuning them for specific tasks.


---->Convolutional Neural Networks (CNN):
->Introduction to CNNs: Understanding the basics of CNNs, convolutional layers, pooling layers, and fully connected layers.
->Convolutional Layers: Exploring the concept of convolutional filters, feature maps, and their role in capturing spatial hierarchies.
->Pooling Layers: Understanding pooling operations like max pooling and average pooling for downsampling and reducing spatial dimensions.
->Stride and Padding: Exploring stride and padding options in CNNs and their impact on output dimensions.
->CNN Architectures: Studying popular CNN architectures like LeNet, AlexNet, VGGNet, GoogLeNet (Inception), ResNet, and their characteristics.
->Transfer Learning with CNNs: Utilizing pre-trained CNN models like VGG16, ResNet, and applying transfer learning for image recognition tasks.
->Data Augmentation: Techniques for augmenting image data to increase the diversity and size of the training set.
Visualizing CNNs: Methods for visualizing and interpreting the learned features and activations within CNNs.



NLP CONCEPTS
1.introduction
2.text preprocessing and cleaning
—removing the stops ,special character handling etc
3.Tokenization techniques algorithms
4.Modeling of nlp N-grams
5.Text classificating and sentimental analysis—use case solving
6.Named Entity Recognition system
—Pos(parts of speech tagging)
—Word embedding
—syntax and parsing
7.RNN
— sequence to sequence modeling concepts
—LSTM model(long short term memory)
—Bidirectional LSTM
—GRU(Gated Recurrent Units)
—Attention mechanisms
———self attention models
———transformers models
———encoder and decoder techniques
———Machine Translation
—BERT algorithms
——Roberta ,alberta,Opensource hugging face models
——Fine tuning the bert models and build a custom nlp applications
—Hugging face models and build a rest apis on top of the open source model
—Langchain introduction and building a custom chatbot 
—6 to 7 usecases —ipynb notebook(practice
—1 end to end project on top of mlops(aws tools)


SATURDAY AND SUNDAY
2 hours and 2 hours==4 hours*4 =16 hours*4 =64 hours
total fees —35k

